{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a113d9-ee30-4991-9608-097c092acfef",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning BERT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371a2f4-91ea-426b-ab63-cf976e06d060",
   "metadata": {},
   "source": [
    "This notebook performs both **standard fine-tuning** and **LoRA (Low-Rank Approximation) fine-tuning** on **BERT models** using custom implementations **coded from scratch**. \n",
    "\n",
    "The fine-tuning **objective** is to optimize the BERT models on the specific task of **predicting** whether a **question submitted** to **StackOverflow** will be **closed**.\n",
    "\n",
    "BERT variants: \n",
    "* **BERT-base**: BERT base uncased\n",
    "* **BERT Overflow**: BERT-base pretrained on 152 million sentences from Stack Overflow\n",
    "* **BERT Tabular**: BERT-base modified to accept tabular features\n",
    "* **LoRA BERT**:\n",
    "    * **rank 1**\n",
    "    * **rank 8**\n",
    "    * **last four layers with rank 8**\n",
    "\n",
    "Dataset: https://www.kaggle.com/competitions/predict-closed-questions-on-stack-overflow/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44620165-32ae-4eeb-87c6-a8e23153ed25",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37892a3d-88e1-45fc-a30c-7f4ec7bb72f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T22:11:28.916687Z",
     "iopub.status.busy": "2023-07-24T22:11:28.916422Z",
     "iopub.status.idle": "2023-07-24T22:11:31.153364Z",
     "shell.execute_reply": "2023-07-24T22:11:31.152689Z",
     "shell.execute_reply.started": "2023-07-24T22:11:28.916667Z"
    }
   },
   "outputs": [],
   "source": [
    "# add python path to include src directory\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# standard library imports\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "# related third party imports\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "# local library specific imports\n",
    "from bert_from_scratch import BertForSequenceClassification as MyBertForSequenceClassification\n",
    "from lora_from_scratch import (\n",
    "    LinearLoRA,\n",
    "    create_lora,\n",
    "    add_lora_layers,\n",
    "    freeze_model,\n",
    "    unfreeze_model,\n",
    "    create_linear,\n",
    "    merge_lora_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaf62c-539f-4f09-abc7-7e6bda535a7e",
   "metadata": {},
   "source": [
    "## STEP 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ccae90f-9179-4b9d-83cf-315889cfe49f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:47:54.736078Z",
     "iopub.status.busy": "2023-07-24T23:47:54.735225Z",
     "iopub.status.idle": "2023-07-24T23:48:27.855876Z",
     "shell.execute_reply": "2023-07-24T23:48:27.855368Z",
     "shell.execute_reply.started": "2023-07-24T23:47:54.736049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b695c052e0ae4be9bea9fe8e206a17a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb0f0bc84d24710993ac4ef716388d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf8e92206a249c2a2ecfcce4e7c37ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\" Instructs how the DataLoader should process the data into a batch\"\"\"\n",
    "    \n",
    "    text = [item['text'] for item in batch]\n",
    "    tabular = torch.stack([torch.tensor(item['tabular']) for item in batch])\n",
    "    labels = torch.stack([torch.tensor(item['label']) for item in batch])\n",
    "\n",
    "    return {'text': text, 'tabular': tabular, 'label': labels}\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/train-sample.csv\")\n",
    "\n",
    "# dict mapping strings to integers\n",
    "string_to_int = {\n",
    "    'open': 0,\n",
    "    'not a real question': 1,\n",
    "    'off topic': 1,\n",
    "    'not constructive': 1,\n",
    "    'too localized': 1\n",
    "}\n",
    "\n",
    "# add new features to dataframe\n",
    "df['OpenStatusInt'] = df['OpenStatus'].map(string_to_int)  # convert class strings to integers\n",
    "df['BodyLength'] = df['BodyMarkdown'].apply(lambda x: len(x.split(\" \")))  # number of words in body text\n",
    "df['TitleLength'] = df['Title'].apply(lambda x: len(x.split(\" \")))  # number of words in title text\n",
    "df['TitleConcatWithBody'] = df.apply(lambda x: x.Title +  \" \" + x.BodyMarkdown, axis=1)  # combine title and body text\n",
    "df['NumberOfTags'] = df.apply(\n",
    "    lambda x: len([x[col] for col in ['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5'] if not pd.isna(x[col])]), \n",
    "    axis=1,\n",
    ")  # number of tags\n",
    "df['PostCreationDate'] = pd.to_datetime(df['PostCreationDate'])  # convert string to Timedelta object\n",
    "df['OwnerCreationDate'] = pd.to_datetime(df['OwnerCreationDate'], format='mixed')  # convert string to Timedelta object\n",
    "df['DayDifference'] = (df['PostCreationDate'] - df['OwnerCreationDate']).dt.days  # days between account creation and post creation \n",
    "\n",
    "# list of col names with tabular data \n",
    "tabular_feature_list = [\n",
    "    'ReputationAtPostCreation',  \n",
    "    'BodyLength', \n",
    "    'TitleLength', \n",
    "    'NumberOfTags',\n",
    "    'DayDifference',\n",
    "]\n",
    "\n",
    "# place the desired data from the dataframe into a dictionary\n",
    "data_dict = {\n",
    "    'text': df.TitleConcatWithBody.tolist(),\n",
    "    'tabular': df[tabular_feature_list].values,\n",
    "    'label': df.OpenStatusInt.tolist(),\n",
    "}\n",
    "\n",
    "# load data into hugging face dataset object\n",
    "dataset_stackoverflow = Dataset.from_dict(data_dict)\n",
    "\n",
    "# define the indices at which to split the dataset into train/validation/test\n",
    "n_samples = len(dataset_stackoverflow)\n",
    "split_idx1 = int(n_samples * 0.8)\n",
    "split_idx2 = int(n_samples * 0.9)\n",
    "\n",
    "# shuffle the dataset\n",
    "shuffled_dataset = dataset_stackoverflow.shuffle(seed=42)\n",
    "\n",
    "# split dataset training/validation/test\n",
    "train_dataset = shuffled_dataset.select(range(split_idx1))\n",
    "val_dataset = shuffled_dataset.select(range(split_idx1, split_idx2))\n",
    "test_dataset = shuffled_dataset.select(range(split_idx2, n_samples))\n",
    "\n",
    "# calculate mean and std of each tabular feature\n",
    "mean_train = torch.mean(torch.tensor(train_dataset['tabular'], dtype=torch.float32), dim=0)\n",
    "std_train = torch.std(torch.tensor(train_dataset['tabular'], dtype=torch.float32), dim=0)\n",
    "\n",
    "# define a function to apply standard scaling to the tabular data\n",
    "def standard_scale(example):\n",
    "    example['tabular'] = (torch.tensor(example['tabular']) - mean_train) / std_train\n",
    "    return example\n",
    "\n",
    "# apply the standard scaling function to the tabular features\n",
    "train_dataset = train_dataset.map(standard_scale)\n",
    "val_dataset = val_dataset.map(standard_scale)\n",
    "test_dataset = test_dataset.map(standard_scale)\n",
    "\n",
    "# load the datasets into a dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6045954-71de-4b71-b0a9-2c3420acbe0e",
   "metadata": {},
   "source": [
    "#### Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcafef1-ca46-4a64-8563-b25de7403bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T22:37:22.569496Z",
     "iopub.status.busy": "2023-07-24T22:37:22.569176Z",
     "iopub.status.idle": "2023-07-24T22:37:22.589597Z",
     "shell.execute_reply": "2023-07-24T22:37:22.588825Z",
     "shell.execute_reply.started": "2023-07-24T22:37:22.569473Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertTrainer:\n",
    "    \"\"\" A training and evaluation loop for PyTorch models with a BERT like architecture. \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataloader,\n",
    "        eval_dataloader=None,\n",
    "        epochs=1,\n",
    "        lr=5e-04,\n",
    "        output_dir='./',\n",
    "        output_filename='model_state_dict.pt',\n",
    "        save=False,\n",
    "        tabular=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: torch.nn.Module: = A PyTorch model with a BERT like architecture,\n",
    "            tokenizer: = A BERT tokenizer for tokenizing text input,\n",
    "            train_dataloader: torch.utils.data.DataLoader = \n",
    "                A dataloader containing the training data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n",
    "            eval_dataloader: torch.utils.data.DataLoader = \n",
    "                A dataloader containing the evaluation data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n",
    "            epochs: int = An integer representing the number epochs to train,\n",
    "            lr: float = A float representing the learning rate for the optimizer,\n",
    "            output_dir: str = A string representing the directory path to save the model,\n",
    "            output_filename: string = A string representing the name of the file to save in the output directory,\n",
    "            save: bool = A boolean representing whether or not to save the model,\n",
    "            tabular: bool = A boolean representing whether or not the BERT model is modified to accept tabular data,\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.output_dir = output_dir\n",
    "        self.output_filename = output_filename\n",
    "        self.save = save\n",
    "        self.eval_loss = float('inf')  # tracks the lowest loss so as to only save the best model  \n",
    "        self.epochs = epochs\n",
    "        self.epoch_best_model = 0  # tracks which epoch the lowest loss is in so as to only save the best model\n",
    "        self.tabular = tabular\n",
    "    \n",
    "    def train(self, evaluate=False):\n",
    "        \"\"\" Calls the batch iterator to train and optionally evaluate the model.\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            self.iteration(epoch, self.train_dataloader)\n",
    "            if evaluate and self.eval_dataloader is not None:\n",
    "                self.iteration(epoch, self.eval_dataloader, train=False)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\" Calls the batch iterator to evaluate the model.\"\"\"\n",
    "        epoch=0\n",
    "        self.iteration(epoch, self.eval_dataloader, train=False)\n",
    "    \n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\" Iterates through one epoch of training or evaluation\"\"\"\n",
    "        \n",
    "        # initialize variables\n",
    "        loss_accumulated = 0.\n",
    "        correct_accumulated = 0\n",
    "        samples_accumulated = 0\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        \n",
    "        self.model.train() if train else self.model.eval()\n",
    "        \n",
    "        # progress bar\n",
    "        mode = \"train\" if train else \"eval\"\n",
    "        batch_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=f\"EP ({mode}) {epoch}\",\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "        \n",
    "        # iterate through batches of the dataset\n",
    "        for i, batch in batch_iter:\n",
    "\n",
    "            # tokenize data\n",
    "            batch_t = self.tokenizer(\n",
    "                batch['text'],\n",
    "                padding='max_length', \n",
    "                max_length=512, \n",
    "                truncation=True,\n",
    "                return_tensors='pt', \n",
    "            )\n",
    "            batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n",
    "            batch_t[\"input_labels\"] = batch[\"label\"].to(self.device)\n",
    "            batch_t[\"tabular_vectors\"] = batch[\"tabular\"].to(self.device)\n",
    "\n",
    "            # forward pass - include tabular data if it is a tabular model\n",
    "            if self.tabular:\n",
    "                logits = self.model(\n",
    "                    input_ids=batch_t[\"input_ids\"], \n",
    "                    token_type_ids=batch_t[\"token_type_ids\"], \n",
    "                    attention_mask=batch_t[\"attention_mask\"],\n",
    "                    tabular_vectors=batch_t[\"tabular_vectors\"],\n",
    "                )   \n",
    "            \n",
    "            else:\n",
    "                logits = self.model(\n",
    "                    input_ids=batch_t[\"input_ids\"], \n",
    "                    token_type_ids=batch_t[\"token_type_ids\"], \n",
    "                    attention_mask=batch_t[\"attention_mask\"],\n",
    "                )\n",
    "\n",
    "            # calculate loss\n",
    "            loss = self.loss_fn(logits, batch_t[\"input_labels\"])\n",
    "    \n",
    "            # compute gradient and and update weights\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # calculate the number of correct predictions\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct = preds.eq(batch_t[\"input_labels\"]).sum().item()\n",
    "            \n",
    "            # accumulate batch metrics and outputs\n",
    "            loss_accumulated += loss.item()\n",
    "            correct_accumulated += correct\n",
    "            samples_accumulated += len(batch_t[\"input_labels\"])\n",
    "            preds_all.append(preds.detach())\n",
    "            labels_all.append(batch_t['input_labels'].detach())\n",
    "        \n",
    "        # concatenate all batch tensors into one tensor and move to cpu for compatibility with sklearn metrics\n",
    "        preds_all = torch.cat(preds_all, dim=0).cpu()\n",
    "        labels_all = torch.cat(labels_all, dim=0).cpu()\n",
    "        \n",
    "        # metrics\n",
    "        accuracy = accuracy_score(labels_all, preds_all)\n",
    "        precision = precision_score(labels_all, preds_all, average='macro')\n",
    "        recall = recall_score(labels_all, preds_all, average='macro')\n",
    "        f1 = f1_score(labels_all, preds_all, average='macro')\n",
    "        avg_loss_epoch = loss_accumulated / len(data_loader)\n",
    "        \n",
    "        # print metrics to console\n",
    "        print(\n",
    "            f\"samples={samples_accumulated}, \\\n",
    "            correct={correct_accumulated}, \\\n",
    "            acc={round(accuracy, 4)}, \\\n",
    "            recall={round(recall, 4)}, \\\n",
    "            prec={round(precision,4)}, \\\n",
    "            f1={round(f1, 4)}, \\\n",
    "            loss={round(avg_loss_epoch, 4)}\"\n",
    "        )    \n",
    "        \n",
    "        # save the model if the evaluation loss is lower than the previous best epoch \n",
    "        if self.save and not train and avg_loss_epoch < self.eval_loss:\n",
    "            \n",
    "            # create directory and filepaths\n",
    "            dir_path = Path(self.output_dir)\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = dir_path / f\"{self.output_filename}_epoch_{epoch}.pt\"\n",
    "            \n",
    "            # delete previous best model from hard drive\n",
    "            if epoch > 0:\n",
    "                file_path_best_model = dir_path / f\"{self.output_filename}_epoch_{self.epoch_best_model}.pt\"\n",
    "                !rm -f $file_path_best_model\n",
    "            \n",
    "            # save model\n",
    "            torch.save({\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            }, file_path)\n",
    "            \n",
    "            # update the new best loss and epoch\n",
    "            self.eval_loss = avg_loss_epoch\n",
    "            self.epoch_best_model = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b7109-b7b4-473b-a81f-8b52e50907bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T22:52:43.827550Z",
     "iopub.status.busy": "2023-07-24T22:52:43.827264Z",
     "iopub.status.idle": "2023-07-24T22:52:43.831180Z",
     "shell.execute_reply": "2023-07-24T22:52:43.830385Z",
     "shell.execute_reply.started": "2023-07-24T22:52:43.827528Z"
    }
   },
   "source": [
    "## STEP 2: Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afc0ff-1ad0-4639-a1bc-f58f183c47ea",
   "metadata": {},
   "source": [
    "### BERT-base\n",
    "\n",
    "#### Instantiate the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f59a3be-870e-47f1-a7ba-41095674f75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T23:08:24.221555Z",
     "iopub.status.busy": "2023-07-05T23:08:24.220927Z",
     "iopub.status.idle": "2023-07-05T23:08:41.137932Z",
     "shell.execute_reply": "2023-07-05T23:08:41.137327Z",
     "shell.execute_reply.started": "2023-07-05T23:08:24.221525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c186442942348cdac8d0dc31fdddacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fe46890cad4b83be80e85a7319b343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbf4f585dd34c82b412626217ed846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c694280461a46c38c8c4fac52173ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and pretrained model\n",
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}  # these are default configs but just added for explicity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b646f85-6b49-4418-8ffd-f87bf341fc2b",
   "metadata": {},
   "source": [
    "#### Fine-Tune the model. \n",
    "\n",
    "The trainer saves the model from the epoch with the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4845b80-9006-4b56-9989-86a6d533b5c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-05T23:11:47.595516Z",
     "iopub.status.busy": "2023-07-05T23:11:47.594460Z",
     "iopub.status.idle": "2023-07-06T03:26:30.922869Z",
     "shell.execute_reply": "2023-07-06T03:26:30.922170Z",
     "shell.execute_reply.started": "2023-07-05T23:11:47.595476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [48:24<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=85975,             acc=0.7661,             recall=0.7662,             prec=0.7663,             f1=0.7661,             loss=0.4903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11013,             acc=0.7851,             recall=0.7853,             prec=0.7856,             f1=0.7851,             loss=0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [48:27<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89764,             acc=0.7999,             recall=0.7999,             prec=0.7999,             f1=0.7999,             loss=0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11034,             acc=0.7866,             recall=0.7869,             prec=0.7877,             f1=0.7865,             loss=0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [48:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=92032,             acc=0.8201,             recall=0.8201,             prec=0.8201,             f1=0.8201,             loss=0.4056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11081,             acc=0.79,             recall=0.7901,             prec=0.7902,             f1=0.79,             loss=0.4624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [48:31<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=94453,             acc=0.8417,             recall=0.8417,             prec=0.8417,             f1=0.8417,             loss=0.3671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:26<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10886,             acc=0.7761,             recall=0.7767,             prec=0.7816,             f1=0.7752,             loss=0.4853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [48:34<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=97186,             acc=0.8661,             recall=0.8661,             prec=0.8661,             f1=0.8661,             loss=0.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:25<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10998,             acc=0.7841,             recall=0.7842,             prec=0.7842,             f1=0.7841,             loss=0.5039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bert base\n",
    "trainer_bert_base = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned',\n",
    "    output_filename='bert_base',\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460c6c4-f187-4568-b0ce-d661c9791091",
   "metadata": {},
   "source": [
    "#### Evaluate the model from the best epoch on the test datset.\n",
    "\n",
    "\n",
    "You have to replace state dictionary path on line 7 in the below cell with the .pt file from the /models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf271c4-ab71-49e5-a2a0-c560689f4e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T23:33:54.369318Z",
     "iopub.status.busy": "2023-07-06T23:33:54.368732Z",
     "iopub.status.idle": "2023-07-06T23:36:24.678915Z",
     "shell.execute_reply": "2023-07-06T23:36:24.678066Z",
     "shell.execute_reply.started": "2023-07-06T23:33:54.369294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:25<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=11022,             acc=0.7857,             recall=0.7855,             prec=0.7862,             f1=0.7855,             loss=0.4594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# copy weights from the saved fine-tuned model\n",
    "state_dict = torch.load('../models/bert_base_fine_tuned/bert_base_epoch_1.pt')  # replace with .pt file from models dir\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "# trainer\n",
    "trainer_bert_base = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned',\n",
    "    output_filename='bert_base',\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "# evaluate on test set\n",
    "trainer_bert_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61384d9-c341-404f-a309-2d4696476970",
   "metadata": {},
   "source": [
    "### BERTOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b605672-7a07-4bf8-9cce-b5322beb017f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T06:56:34.586919Z",
     "iopub.status.busy": "2023-07-06T06:56:34.586104Z",
     "iopub.status.idle": "2023-07-06T06:56:54.363583Z",
     "shell.execute_reply": "2023-07-06T06:56:54.363033Z",
     "shell.execute_reply.started": "2023-07-06T06:56:34.586894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204efffc5a20460d9f6c34a3c7d27b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/644k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878471197ec1455982492d1b64e04bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4fcde768e744fa877b318a75886d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb532365d5e4838b3d66c422c34f170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/3.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: lanwuwei/BERTOverflow_stackoverflow_github\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a52dac693254a2a9f7b5f5b9447d948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/568M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at lanwuwei/BERTOverflow_stackoverflow_github and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_overflow = BertTokenizer.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')\n",
    "bert_overflow = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='lanwuwei/BERTOverflow_stackoverflow_github', \n",
    "    config_args={\"vocab_size\": 82000, \"n_classes\": 2, \"pad_token_id\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05eb7428-cedc-445c-a0c2-3e6663c8ff5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T06:57:46.650273Z",
     "iopub.status.busy": "2023-07-06T06:57:46.649440Z",
     "iopub.status.idle": "2023-07-06T11:06:45.214806Z",
     "shell.execute_reply": "2023-07-06T11:06:45.213900Z",
     "shell.execute_reply.started": "2023-07-06T06:57:46.650248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [47:24<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=78125,             acc=0.6962,             recall=0.6962,             prec=0.6967,             f1=0.696,             loss=0.5828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:17<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10303,             acc=0.7345,             recall=0.7347,             prec=0.7352,             f1=0.7344,             loss=0.5331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [47:25<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=84117,             acc=0.7496,             recall=0.7496,             prec=0.7496,             f1=0.7496,             loss=0.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:17<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10500,             acc=0.7486,             recall=0.7487,             prec=0.749,             f1=0.7485,             loss=0.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [47:27<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=86506,             acc=0.7709,             recall=0.7709,             prec=0.7709,             f1=0.7709,             loss=0.4854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:17<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10605,             acc=0.756,             recall=0.7562,             prec=0.7564,             f1=0.756,             loss=0.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [47:28<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=88250,             acc=0.7864,             recall=0.7864,             prec=0.7864,             f1=0.7864,             loss=0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:18<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10627,             acc=0.7576,             recall=0.7574,             prec=0.7579,             f1=0.7574,             loss=0.5025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [47:27<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89663,             acc=0.799,             recall=0.799,             prec=0.799,             f1=0.799,             loss=0.439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:18<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10459,             acc=0.7456,             recall=0.7464,             prec=0.7539,             f1=0.7439,             loss=0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bert overflow\n",
    "trainer_bert_overflow = BertTrainer(\n",
    "    bert_overflow,\n",
    "    tokenizer_overflow,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_overflow_fine_tuned',\n",
    "    output_filename='bert_overflow',\n",
    "    save=True\n",
    ")\n",
    "\n",
    "trainer_bert_overflow.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7d6347-5a4b-49d5-90a4-350783bce721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-06T23:30:56.482276Z",
     "iopub.status.busy": "2023-07-06T23:30:56.481472Z",
     "iopub.status.idle": "2023-07-06T23:33:16.262927Z",
     "shell.execute_reply": "2023-07-06T23:33:16.262412Z",
     "shell.execute_reply.started": "2023-07-06T23:30:56.482252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:18<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=10685,             acc=0.7617,             recall=0.7619,             prec=0.7624,             f1=0.7616,             loss=0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('../models/bert_overflow_fine_tuned/bert_overflow_epoch_3.pt')\n",
    "bert_overflow.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "trainer_bert_overflow = BertTrainer(\n",
    "    bert_overflow,\n",
    "    tokenizer_overflow,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_overflow_fine_tuned',\n",
    "    output_filename='bert_overflow',\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "trainer_bert_overflow.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4e6a4-6439-4f01-a957-cdc1211c5537",
   "metadata": {},
   "source": [
    "### BERT Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce05a76-102c-46a6-805d-7356f866d9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-20T18:19:47.064082Z",
     "iopub.status.busy": "2023-07-20T18:19:47.063882Z",
     "iopub.status.idle": "2023-07-20T18:19:47.068906Z",
     "shell.execute_reply": "2023-07-20T18:19:47.068267Z",
     "shell.execute_reply.started": "2023-07-20T18:19:47.064063Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularBertForSequenceClassification(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A BERT model for sequence classification that concatenates a vector of tabular features\n",
    "    to the pooler output (pooled last hidden state) before pushing to the classification head.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_model, tabular_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bert_model: torch.nn.Module: = A PyTorch model with a BERT like architecture that returns the pooled embedding\n",
    "            tabular_size: int = Integer representing the size of the tabular vector to concatenate   \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.emb_size = self.bert_model.config.emb_size\n",
    "        self.n_classes = self.bert_model.config.n_classes\n",
    "        self.classifier_replacement = torch.nn.Linear(self.emb_size + tabular_size, self.n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, tabular_vectors, attention_mask=None):\n",
    "        pooler_out, _ = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        pooler_out_cat_tabular = torch.cat((pooler_out, tabular_vectors), dim=-1)\n",
    "        out = self.classifier_replacement(pooler_out_cat_tabular)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6e104d-f823-4e5e-8fb4-1c2a12f1ab09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-20T18:20:11.780294Z",
     "iopub.status.busy": "2023-07-20T18:20:11.779891Z",
     "iopub.status.idle": "2023-07-20T18:20:19.774088Z",
     "shell.execute_reply": "2023-07-20T18:20:19.773372Z",
     "shell.execute_reply.started": "2023-07-20T18:20:11.780260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683f9d4002e14f7eb8011436abd5e2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a4a41dea9147439814482c26581fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae88da300a8446d8c070291283cac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebb668d85c24e20994d47efae28151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2, \"return_pooler_output\": True}\n",
    ")\n",
    "\n",
    "tabular_size = len(train_dataset['tabular'][0])\n",
    "bert_base_tabular = TabularBertForSequenceClassification(bert_base, tabular_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc78d01-d9ed-4161-a6ee-b37afb94d9e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-20T18:24:05.719843Z",
     "iopub.status.busy": "2023-07-20T18:24:05.719557Z",
     "iopub.status.idle": "2023-07-20T22:37:19.234589Z",
     "shell.execute_reply": "2023-07-20T22:37:19.233937Z",
     "shell.execute_reply.started": "2023-07-20T18:24:05.719821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [48:13<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=86075,             acc=0.767,             recall=0.767,             prec=0.7671,             f1=0.767,             loss=0.4894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:24<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10972,             acc=0.7822,             recall=0.7821,             prec=0.7823,             f1=0.7821,             loss=0.4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [48:13<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89652,             acc=0.7989,             recall=0.7989,             prec=0.799,             f1=0.7989,             loss=0.4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:24<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11103,             acc=0.7915,             recall=0.7916,             prec=0.7915,             f1=0.7915,             loss=0.4561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [48:12<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=91891,             acc=0.8189,             recall=0.8189,             prec=0.8189,             f1=0.8189,             loss=0.4073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:24<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11075,             acc=0.7895,             recall=0.7893,             prec=0.7904,             f1=0.7893,             loss=0.4566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [48:13<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=94457,             acc=0.8417,             recall=0.8417,             prec=0.8417,             f1=0.8417,             loss=0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:24<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=11014,             acc=0.7852,             recall=0.7848,             prec=0.7867,             f1=0.7848,             loss=0.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [48:10<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=97005,             acc=0.8644,             recall=0.8644,             prec=0.8644,             f1=0.8644,             loss=0.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:25<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10966,             acc=0.7818,             recall=0.7815,             prec=0.7829,             f1=0.7814,             loss=0.5054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bert base tabular\n",
    "trainer_bert_base_tabular = BertTrainer(\n",
    "    bert_base_tabular,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_tabular',\n",
    "    output_filename='bert_base_tabular',\n",
    "    save=True,\n",
    "    tabular=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base_tabular.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba23ea16-2748-45cf-9653-c8c58ce9b084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-20T22:44:43.919015Z",
     "iopub.status.busy": "2023-07-20T22:44:43.918191Z",
     "iopub.status.idle": "2023-07-20T22:47:17.643361Z",
     "shell.execute_reply": "2023-07-20T22:47:17.642532Z",
     "shell.execute_reply.started": "2023-07-20T22:44:43.918983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 1754/1754 [02:33<00:00, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=11064,             acc=0.7887,             recall=0.7887,             prec=0.7887,             f1=0.7887,             loss=0.4558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('../models/bert_base_fine_tuned_tabular/bert_base_tabular_epoch_1.pt')\n",
    "bert_base_tabular.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "trainer_bert_base_tabular = BertTrainer(\n",
    "    bert_base_tabular,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_tabular',\n",
    "    output_filename='bert_base_tabular',\n",
    "    save=False,\n",
    "    tabular=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base_tabular.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727187e-669a-45a0-9370-87042f1a3618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T07:04:52.740074Z",
     "iopub.status.busy": "2023-07-07T07:04:52.739507Z",
     "iopub.status.idle": "2023-07-07T07:04:52.753494Z",
     "shell.execute_reply": "2023-07-07T07:04:52.752928Z",
     "shell.execute_reply.started": "2023-07-07T07:04:52.740049Z"
    }
   },
   "source": [
    "### LoRA BERT-base\n",
    "#### rank = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b1890c-40ad-4e05-af6c-bf213734ddf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:19:09.125052Z",
     "iopub.status.busy": "2023-07-24T23:19:09.124454Z",
     "iopub.status.idle": "2023-07-24T23:19:11.172040Z",
     "shell.execute_reply": "2023-07-24T23:19:11.171438Z",
     "shell.execute_reply.started": "2023-07-24T23:19:09.125018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}  # these are default configs but just added for explicity\n",
    ")\n",
    "\n",
    "add_lora_layers(bert_base, r=8, lora_alpha=16)  # inject the LoRA layers into the model\n",
    "freeze_model(bert_base)  # freeze the non-LoRA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c1b9-1bfb-4d66-b619-51eb1fd63209",
   "metadata": {},
   "source": [
    "Count how many of the parameters in the model are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9e380c-abc9-4303-8e61-45aec24ac4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:24:08.706451Z",
     "iopub.status.busy": "2023-07-24T23:24:08.705940Z",
     "iopub.status.idle": "2023-07-24T23:24:08.711337Z",
     "shell.execute_reply": "2023-07-24T23:24:08.710699Z",
     "shell.execute_reply.started": "2023-07-24T23:24:08.706430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109778690\n",
      "Trainable parameters: 296450\n",
      "Percentage trainable: 0.27%\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "n_trainable_params = 0\n",
    "\n",
    "# count the number of trainable parameters\n",
    "for n, p in bert_base.named_parameters():\n",
    "    n_params += p.numel()\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"Trainable parameters: {n_trainable_params}\")\n",
    "print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b90b17-792a-4637-b3c4-a009eac57551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T02:52:53.840874Z",
     "iopub.status.busy": "2023-07-07T02:52:53.840080Z",
     "iopub.status.idle": "2023-07-07T06:18:12.433645Z",
     "shell.execute_reply": "2023-07-07T06:18:12.433047Z",
     "shell.execute_reply.started": "2023-07-07T02:52:53.840839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [38:32<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=86105,             acc=0.7673,             recall=0.7673,             prec=0.7674,             f1=0.7673,             loss=0.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10730,             acc=0.765,             recall=0.764,             prec=0.776,             f1=0.7621,             loss=0.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [38:33<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89196,             acc=0.7949,             recall=0.7949,             prec=0.7949,             f1=0.7949,             loss=0.4482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:29<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10908,             acc=0.7776,             recall=0.7777,             prec=0.7777,             f1=0.7776,             loss=0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [38:33<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=90999,             acc=0.8109,             recall=0.8109,             prec=0.8109,             f1=0.8109,             loss=0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10986,             acc=0.7832,             recall=0.7834,             prec=0.7839,             f1=0.7831,             loss=0.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [38:35<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=92469,             acc=0.824,             recall=0.824,             prec=0.824,             f1=0.824,             loss=0.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:29<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10937,             acc=0.7797,             recall=0.7799,             prec=0.7802,             f1=0.7797,             loss=0.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [38:34<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=94023,             acc=0.8379,             recall=0.8379,             prec=0.8379,             f1=0.8379,             loss=0.3718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:28<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10862,             acc=0.7744,             recall=0.7745,             prec=0.7747,             f1=0.7743,             loss=0.5011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bert base lora all r = 8\n",
    "trainer_bert_base_lora = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-04,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8',\n",
    "    output_filename='bert_base_lora_r8',\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf3d05f-49a1-44e6-ad01-d0299180cfb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:29:31.001180Z",
     "iopub.status.busy": "2023-07-24T23:29:31.000545Z",
     "iopub.status.idle": "2023-07-24T23:29:31.004613Z",
     "shell.execute_reply": "2023-07-24T23:29:31.003903Z",
     "shell.execute_reply.started": "2023-07-24T23:29:31.001149Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/bert_base_fine_tuned_lora_r8/bert_base_lora_r8_epoch_1.pt\")\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "# merge weights\n",
    "merge_lora_layers(bert_base) \n",
    "unfreeze_model(bert_base)\n",
    "\n",
    "# create directory and filepaths\n",
    "dir_path = Path(\"../models/bert_base_fine_tuned_lora_r8/merged\")\n",
    "dir_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = dir_path / \"bert_base_lora_r8_epoch_1_merged.pt\"\n",
    "\n",
    "# save model\n",
    "torch.save({\n",
    "    \"model_state_dict\": bert_base.state_dict(),\n",
    "}, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc973148-5ec8-4576-ab18-d5c2a7a59da4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T06:56:55.460915Z",
     "iopub.status.busy": "2023-07-07T06:56:55.460140Z",
     "iopub.status.idle": "2023-07-07T06:59:18.430295Z",
     "shell.execute_reply": "2023-07-07T06:59:18.429834Z",
     "shell.execute_reply.started": "2023-07-07T06:56:55.460890Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:22<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=10985,             acc=0.7831,             recall=0.7831,             prec=0.7831,             f1=0.7831,             loss=0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base_lora_r8 = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8',\n",
    "    output_filename='bert_base_lora_r8',\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora_r8.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fdc16-5f9f-408b-8d69-5e27992476ec",
   "metadata": {},
   "source": [
    "### LoRA BERT-base\n",
    "#### rank = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3855a193-a204-4114-80d8-0395ddf16241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:33:42.646538Z",
     "iopub.status.busy": "2023-07-24T23:33:42.645757Z",
     "iopub.status.idle": "2023-07-24T23:33:44.692824Z",
     "shell.execute_reply": "2023-07-24T23:33:44.692219Z",
     "shell.execute_reply.started": "2023-07-24T23:33:42.646512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}  # these are default configs but just added for explicity\n",
    ")\n",
    "\n",
    "add_lora_layers(bert_base, r=1, lora_alpha=2)\n",
    "freeze_model(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2990f34e-e055-4ea0-a202-9114d18032af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:33:47.995346Z",
     "iopub.status.busy": "2023-07-24T23:33:47.994955Z",
     "iopub.status.idle": "2023-07-24T23:33:48.001386Z",
     "shell.execute_reply": "2023-07-24T23:33:48.000740Z",
     "shell.execute_reply.started": "2023-07-24T23:33:47.995320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109520642\n",
      "Trainable parameters: 38402\n",
      "Percentage trainable: 0.04%\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "n_trainable_params = 0\n",
    "\n",
    "# count the number of trainable parameters\n",
    "for n, p in bert_base.named_parameters():\n",
    "    n_params += p.numel()\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"Trainable parameters: {n_trainable_params}\")\n",
    "print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fe20e8-525e-4f1e-beab-641ec9de41ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T07:05:23.817439Z",
     "iopub.status.busy": "2023-07-07T07:05:23.816831Z",
     "iopub.status.idle": "2023-07-07T10:29:56.843290Z",
     "shell.execute_reply": "2023-07-07T10:29:56.842706Z",
     "shell.execute_reply.started": "2023-07-07T07:05:23.817414Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [38:25<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=85181,             acc=0.7591,             recall=0.7591,             prec=0.7592,             f1=0.7591,             loss=0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:27<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10844,             acc=0.7731,             recall=0.7732,             prec=0.7733,             f1=0.7731,             loss=0.4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [38:25<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=88112,             acc=0.7852,             recall=0.7852,             prec=0.7852,             f1=0.7852,             loss=0.4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:28<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10799,             acc=0.7699,             recall=0.7706,             prec=0.7767,             f1=0.7688,             loss=0.4891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [38:24<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89344,             acc=0.7962,             recall=0.7962,             prec=0.7962,             f1=0.7962,             loss=0.4453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:28<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10937,             acc=0.7797,             recall=0.7798,             prec=0.7798,             f1=0.7797,             loss=0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [38:22<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=90204,             acc=0.8038,             recall=0.8038,             prec=0.8038,             f1=0.8038,             loss=0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:28<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10917,             acc=0.7783,             recall=0.7781,             prec=0.7788,             f1=0.7781,             loss=0.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [38:27<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=91059,             acc=0.8115,             recall=0.8115,             prec=0.8115,             f1=0.8115,             loss=0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:28<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10891,             acc=0.7764,             recall=0.7764,             prec=0.7764,             f1=0.7764,             loss=0.4778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bert base lora all r = 1\n",
    "trainer_bert_base_lora_r1 = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-04,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r1',\n",
    "    output_filename='bert_base_lora_r1',\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora_r1.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7be24a-849d-430a-b58c-9968101de424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T15:48:22.875067Z",
     "iopub.status.busy": "2023-07-07T15:48:22.874625Z",
     "iopub.status.idle": "2023-07-07T15:48:24.922986Z",
     "shell.execute_reply": "2023-07-07T15:48:24.922401Z",
     "shell.execute_reply.started": "2023-07-07T15:48:22.875042Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/bert_base_fine_tuned_lora_r1/bert_base_lora_r1_epoch_2.pt\")\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "# merge weights\n",
    "merge_lora_layers(bert_base)\n",
    "unfreeze_model(bert_base)\n",
    "\n",
    "# create directory and filepaths\n",
    "dir_path = Path(\"../models/bert_base_fine_tuned_lora_r1/merged\")\n",
    "dir_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = dir_path / \"bert_base_lora_r1_epoch_2_merged.pt\"\n",
    "\n",
    "# save model\n",
    "torch.save({\n",
    "    \"model_state_dict\": bert_base.state_dict(),\n",
    "}, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81015bb4-b9ec-4eb9-aae5-0ca2884c9441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T15:48:30.002105Z",
     "iopub.status.busy": "2023-07-07T15:48:30.001581Z",
     "iopub.status.idle": "2023-07-07T15:51:39.919404Z",
     "shell.execute_reply": "2023-07-07T15:51:39.918683Z",
     "shell.execute_reply.started": "2023-07-07T15:48:45.165386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 1754/1754 [02:54<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=10954,             acc=0.7809,             recall=0.7808,             prec=0.7809,             f1=0.7808,             loss=0.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base_lora_r1 = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r1',\n",
    "    output_filename='bert_base_lora_r1',\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora_r1.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef2c53-582e-4613-8819-68f29b71f725",
   "metadata": {},
   "source": [
    "### LoRA BERT-base\n",
    "#### Last 4 layers only rank = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c34f867a-19aa-4bf3-9937-2225f625c234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:38:12.215602Z",
     "iopub.status.busy": "2023-07-24T23:38:12.214939Z",
     "iopub.status.idle": "2023-07-24T23:38:14.007529Z",
     "shell.execute_reply": "2023-07-24T23:38:14.007006Z",
     "shell.execute_reply.started": "2023-07-24T23:38:12.215572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_base = MyBertForSequenceClassification.from_pretrained(\n",
    "    model_type='bert-base-uncased',\n",
    "    config_args={\"vocab_size\": 30522, \"n_classes\": 2}  # these are default configs but just added for explicity\n",
    ")\n",
    "\n",
    "add_lora_layers(bert_base, r=8, lora_alpha=16, ignore_layers=[0,1,2,3,4,5,6,7])\n",
    "freeze_model(bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cba0b841-65c8-43c1-9a81-7ec78dd3b578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-24T23:38:17.003606Z",
     "iopub.status.busy": "2023-07-24T23:38:17.003149Z",
     "iopub.status.idle": "2023-07-24T23:38:17.008377Z",
     "shell.execute_reply": "2023-07-24T23:38:17.007931Z",
     "shell.execute_reply.started": "2023-07-24T23:38:17.003583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109582082\n",
      "Trainable parameters: 99842\n",
      "Percentage trainable: 0.09%\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "n_trainable_params = 0\n",
    "\n",
    "# count the number of trainable parameters\n",
    "for n, p in bert_base.named_parameters():\n",
    "    n_params += p.numel()\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"Trainable parameters: {n_trainable_params}\")\n",
    "print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7312310c-ae32-4d95-b15b-e2a87bdbdd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T10:29:59.005301Z",
     "iopub.status.busy": "2023-07-07T10:29:59.005117Z",
     "iopub.status.idle": "2023-07-07T12:49:39.553682Z",
     "shell.execute_reply": "2023-07-07T12:49:39.552987Z",
     "shell.execute_reply.started": "2023-07-07T10:29:59.005284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 0: 100%|| 3507/3507 [25:33<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=85706,             acc=0.7638,             recall=0.7638,             prec=0.7638,             f1=0.7637,             loss=0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 439/439 [02:26<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10821,             acc=0.7714,             recall=0.7713,             prec=0.7717,             f1=0.7713,             loss=0.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 1: 100%|| 3507/3507 [25:27<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=88350,             acc=0.7873,             recall=0.7873,             prec=0.7873,             f1=0.7873,             loss=0.4602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 1: 100%|| 439/439 [02:25<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10911,             acc=0.7779,             recall=0.7773,             prec=0.7822,             f1=0.7767,             loss=0.4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 2: 100%|| 3507/3507 [25:27<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=89349,             acc=0.7962,             recall=0.7962,             prec=0.7962,             f1=0.7962,             loss=0.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 2: 100%|| 439/439 [02:25<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10958,             acc=0.7812,             recall=0.7813,             prec=0.7814,             f1=0.7812,             loss=0.4718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 3: 100%|| 3507/3507 [25:29<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=90434,             acc=0.8059,             recall=0.8059,             prec=0.8059,             f1=0.8059,             loss=0.4287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 3: 100%|| 439/439 [02:26<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10912,             acc=0.7779,             recall=0.7785,             prec=0.7819,             f1=0.7774,             loss=0.4825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (train) 4: 100%|| 3507/3507 [25:28<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=112217,             correct=91471,             acc=0.8151,             recall=0.8151,             prec=0.8151,             f1=0.8151,             loss=0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 4: 100%|| 439/439 [02:25<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14027,             correct=10928,             acc=0.7791,             recall=0.7789,             prec=0.7793,             f1=0.7789,             loss=0.4691\n"
     ]
    }
   ],
   "source": [
    "#bert base lora rank 8 last 4 layers\n",
    "trainer_bert_base_lora_r8_last_4_layers = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-04,\n",
    "    epochs=5,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=val_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8_last_4_layers',\n",
    "    output_filename='bert_base_lora_r8_last_4_layers',\n",
    "    save=True,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora_r8_last_4_layers.train(evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e32938-1b79-4050-8448-b0e761b5c391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T15:56:25.306823Z",
     "iopub.status.busy": "2023-07-07T15:56:25.306290Z",
     "iopub.status.idle": "2023-07-07T15:56:27.295807Z",
     "shell.execute_reply": "2023-07-07T15:56:27.294934Z",
     "shell.execute_reply.started": "2023-07-07T15:56:25.306793Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/bert_base_fine_tuned_lora_r8_last_4_layers/bert_base_lora_r8_last_4_layers_epoch_4.pt\")\n",
    "bert_base.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\n",
    "merge_lora_layers(bert_base)\n",
    "unfreeze_model(bert_base)\n",
    "\n",
    "# create directory and filepaths\n",
    "dir_path = Path(\"../models/bert_base_fine_tuned_lora_r8_last_4_layers/merged\")\n",
    "dir_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = dir_path / \"bert_base_lora_r8_last_4_layers_epoch_4_merged.pt\"\n",
    "\n",
    "# save model\n",
    "torch.save({\n",
    "    \"model_state_dict\": bert_base.state_dict(),\n",
    "}, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6ade4f-2e05-4599-bd11-b0ff4c69d88d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T15:57:27.246264Z",
     "iopub.status.busy": "2023-07-07T15:57:27.245984Z",
     "iopub.status.idle": "2023-07-07T16:00:21.049791Z",
     "shell.execute_reply": "2023-07-07T16:00:21.049151Z",
     "shell.execute_reply.started": "2023-07-07T15:57:27.246244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP (eval) 0: 100%|| 1754/1754 [02:53<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples=14028,             correct=10958,             acc=0.7812,             recall=0.7813,             prec=0.7817,             f1=0.7811,             loss=0.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_bert_base_lora_r8_last_4_layers = BertTrainer(\n",
    "    bert_base,\n",
    "    tokenizer_base,\n",
    "    lr=5e-06,\n",
    "    epochs=5,\n",
    "    train_dataloader=test_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    output_dir='../models/bert_base_fine_tuned_lora_r8_last_4_layers',\n",
    "    output_filename='bert_base_lora_r8_last_4_layers',\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "trainer_bert_base_lora_r8_last_4_layers.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
